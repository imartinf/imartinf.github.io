
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
        <link rel="canonical" href="https://imartinf.github.io/pubs/">
      
      
        <link rel="prev" href="../contact/">
      
      
        <link rel="next" href="../slides/">
      
      
      <link rel="icon" href="../assets/logo.svg">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.6.22">
    
    
      
        <title>Publications - Iván Martín-Fernández</title>
      
    
    
      <link rel="stylesheet" href="../assets/stylesheets/main.84d31ad4.min.css">
      
        
        <link rel="stylesheet" href="../assets/stylesheets/palette.06af60db.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Inter:300,300i,400,400i,700,700i%7CFragment+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Inter";--md-code-font:"Fragment Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../stylesheets/extra.css">
    
    <script>__md_scope=new URL("..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
      
        <meta  property="og:type"  content="website" >
      
        <meta  property="og:title"  content="Publications - Iván Martín-Fernández" >
      
        <meta  property="og:description"  content="None" >
      
        <meta  property="og:image"  content="https://imartinf.github.io/assets/images/social/pubs.png" >
      
        <meta  property="og:image:type"  content="image/png" >
      
        <meta  property="og:image:width"  content="1200" >
      
        <meta  property="og:image:height"  content="630" >
      
        <meta  property="og:url"  content="https://imartinf.github.io/pubs/" >
      
        <meta  name="twitter:card"  content="summary_large_image" >
      
        <meta  name="twitter:title"  content="Publications - Iván Martín-Fernández" >
      
        <meta  name="twitter:description"  content="None" >
      
        <meta  name="twitter:image"  content="https://imartinf.github.io/assets/images/social/pubs.png" >
      
    
    
  </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="personal" data-md-color-primary="custom" data-md-color-accent="custom">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#publications" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href=".." title="Iván Martín-Fernández" class="md-header__button md-logo" aria-label="Iván Martín-Fernández" data-md-component="logo">
      
  <img src="../assets/logo.svg" alt="logo">

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Iván Martín-Fernández
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Publications
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
  
    
    
    
    <input class="md-option" data-md-color-media="(prefers-color-scheme: light)" data-md-color-scheme="personal" data-md-color-primary="custom" data-md-color-accent="custom"  aria-label="Switch to dark mode"  type="radio" name="__palette" id="__palette_0">
    
      <label class="md-header__button md-icon" title="Switch to dark mode" for="__palette_1" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a4 4 0 0 0-4 4 4 4 0 0 0 4 4 4 4 0 0 0 4-4 4 4 0 0 0-4-4m0 10a6 6 0 0 1-6-6 6 6 0 0 1 6-6 6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"/></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="(prefers-color-scheme: dark)" data-md-color-scheme="slate" data-md-color-primary="custom" data-md-color-accent="custom"  aria-label="Switch to light mode"  type="radio" name="__palette" id="__palette_1">
    
      <label class="md-header__button md-icon" title="Switch to light mode" for="__palette_0" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 18c-.89 0-1.74-.2-2.5-.55C11.56 16.5 13 14.42 13 12s-1.44-4.5-3.5-5.45C10.26 6.2 11.11 6 12 6a6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"/></svg>
      </label>
    
  
</form>
      
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    



<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href=".." title="Iván Martín-Fernández" class="md-nav__button md-logo" aria-label="Iván Martín-Fernández" data-md-component="logo">
      
  <img src="../assets/logo.svg" alt="logo">

    </a>
    Iván Martín-Fernández
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href=".." class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    It's me! Hi!
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../contact/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Contact
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  
  <span class="md-ellipsis">
    Publications
    
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  
  <span class="md-ellipsis">
    Publications
    
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#2025" class="md-nav__link">
    <span class="md-ellipsis">
      2025
    </span>
  </a>
  
    <nav class="md-nav" aria-label="2025">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#parameter-efficient-adaptation-of-large-visionlanguage-models-for-video-memorability-prediction" class="md-nav__link">
    <span class="md-ellipsis">
      Parameter-Efficient Adaptation of Large Vision—Language Models for Video Memorability Prediction
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#2024" class="md-nav__link">
    <span class="md-ellipsis">
      2024
    </span>
  </a>
  
    <nav class="md-nav" aria-label="2024">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#larger-encoders-smaller-regressors-exploring-label-dimensionality-reduction-and-multimodal-large-language-models-as-feature-extractors-for-predicting-social-perception" class="md-nav__link">
    <span class="md-ellipsis">
      Larger Encoders, Smaller Regressors: Exploring Label Dimensionality Reduction and Multimodal Large Language Models as Feature Extractors for Predicting Social Perception
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#2023" class="md-nav__link">
    <span class="md-ellipsis">
      2023
    </span>
  </a>
  
    <nav class="md-nav" aria-label="2023">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#exploring-video-transformers-and-automatic-segment-selection-for-memorability-prediciton" class="md-nav__link">
    <span class="md-ellipsis">
      Exploring Video Transformers and Automatic Segment Selection for Memorability Prediciton
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#video-memorability-prediction-from-jointly-learnt-semantic-and-visual-features" class="md-nav__link">
    <span class="md-ellipsis">
      Video Memorability Prediction From Jointly-learnt Semantic and Visual Features
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../slides/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Slides
    
  </span>
  

      </a>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#2025" class="md-nav__link">
    <span class="md-ellipsis">
      2025
    </span>
  </a>
  
    <nav class="md-nav" aria-label="2025">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#parameter-efficient-adaptation-of-large-visionlanguage-models-for-video-memorability-prediction" class="md-nav__link">
    <span class="md-ellipsis">
      Parameter-Efficient Adaptation of Large Vision—Language Models for Video Memorability Prediction
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#2024" class="md-nav__link">
    <span class="md-ellipsis">
      2024
    </span>
  </a>
  
    <nav class="md-nav" aria-label="2024">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#larger-encoders-smaller-regressors-exploring-label-dimensionality-reduction-and-multimodal-large-language-models-as-feature-extractors-for-predicting-social-perception" class="md-nav__link">
    <span class="md-ellipsis">
      Larger Encoders, Smaller Regressors: Exploring Label Dimensionality Reduction and Multimodal Large Language Models as Feature Extractors for Predicting Social Perception
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#2023" class="md-nav__link">
    <span class="md-ellipsis">
      2023
    </span>
  </a>
  
    <nav class="md-nav" aria-label="2023">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#exploring-video-transformers-and-automatic-segment-selection-for-memorability-prediciton" class="md-nav__link">
    <span class="md-ellipsis">
      Exploring Video Transformers and Automatic Segment Selection for Memorability Prediciton
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#video-memorability-prediction-from-jointly-learnt-semantic-and-visual-features" class="md-nav__link">
    <span class="md-ellipsis">
      Video Memorability Prediction From Jointly-learnt Semantic and Visual Features
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  



<h1 id="publications">Publications</h1>
<h2 id="2025">2025</h2>
<h3 id="parameter-efficient-adaptation-of-large-visionlanguage-models-for-video-memorability-prediction">Parameter-Efficient Adaptation of Large Vision—Language Models for Video Memorability Prediction</h3>
<p>Iván Martín-Fernández, Sergio Esteban-Romero, Fernando Fernández-Martínez, and Manuel Gil-Martín. 2025. "Parameter-Efficient Adaptation of Large Vision—Language Models for Video Memorability Prediction" Sensors 25, no. 6: 1661. <a href="https://doi.org/10.3390/s25061661">https://doi.org/10.3390/s25061661</a></p>
<p><strong>Abstract</strong>
The accurate modelling of video memorability, or the intrinsic properties that render a piece of audiovisual content more likely to be remembered, will facilitate the development of automatic systems that are more efficient in retrieving, classifying and generating impactful media. Recent studies have indicated a strong correlation between the visual semantics of video and its memorability. This underscores the importance of developing advanced visual comprehension abilities to enhance model performance. It has been demonstrated that Large Vision–Language Models (LVLMs) demonstrate exceptional proficiency in generalist, high-level semantic comprehension of images and video, due to their extensive multimodal pre-training on a vast scale. This work makes use of the vast generalist knowledge of LVLMs and explores efficient adaptation techniques with a view to utilising them as memorability predictors. In particular, the Quantized Low-Rank Adaptation (QLoRA) technique is employed to fine-tune the Qwen-VL model with memorability-related data extracted from the Memento10k dataset. In light of existing research, we propose a particular methodology that transforms Qwen-VL from a language model to a memorability score regressor. Furthermore, we consider the influence of selecting appropriate LoRA hyperparameters, a design aspect that has been insufficiently studied. We validate the LoRA rank and alpha hyperparameters using 5-Fold Cross-Validation and evaluate our best configuration on the official testing portion of the Memento10k dataset, obtaining a state-of-the-art Spearman Rank Correlation Coefficient (SRCC) of 0.744. Consequently, this work represents a significant advancement in modelling video memorability through high-level semantic understanding.</p>
<div class="highlight"><span class="filename">Bibtex</span><pre><span></span><code><a id="__codelineno-0-1" name="__codelineno-0-1" href="#__codelineno-0-1"></a>@article{s25061661,
<a id="__codelineno-0-2" name="__codelineno-0-2" href="#__codelineno-0-2"></a>    article-number = {1661},
<a id="__codelineno-0-3" name="__codelineno-0-3" href="#__codelineno-0-3"></a>    author = {Mart{\&#39;\i}n-Fern{\&#39;a}ndez, Iv{\&#39;a}n and Esteban-Romero, Sergio and Fern{\&#39;a}ndez-Mart{\&#39;\i}nez, Fernando and Gil-Mart{\&#39;\i}n, Manuel},
<a id="__codelineno-0-4" name="__codelineno-0-4" href="#__codelineno-0-4"></a>    doi = {10.3390/s25061661},
<a id="__codelineno-0-5" name="__codelineno-0-5" href="#__codelineno-0-5"></a>    issn = {1424-8220},
<a id="__codelineno-0-6" name="__codelineno-0-6" href="#__codelineno-0-6"></a>    journal = {Sensors},
<a id="__codelineno-0-7" name="__codelineno-0-7" href="#__codelineno-0-7"></a>    number = {6},
<a id="__codelineno-0-8" name="__codelineno-0-8" href="#__codelineno-0-8"></a>    pubmedid = {40292713},
<a id="__codelineno-0-9" name="__codelineno-0-9" href="#__codelineno-0-9"></a>    title = {Parameter-Efficient Adaptation of Large Vision---Language Models for Video Memorability Prediction},
<a id="__codelineno-0-10" name="__codelineno-0-10" href="#__codelineno-0-10"></a>    url = {https://www.mdpi.com/1424-8220/25/6/1661},
<a id="__codelineno-0-11" name="__codelineno-0-11" href="#__codelineno-0-11"></a>    volume = {25},
<a id="__codelineno-0-12" name="__codelineno-0-12" href="#__codelineno-0-12"></a>    year = {2025},
<a id="__codelineno-0-13" name="__codelineno-0-13" href="#__codelineno-0-13"></a>    bdsk-url-1 = {https://www.mdpi.com/1424-8220/25/6/1661},
<a id="__codelineno-0-14" name="__codelineno-0-14" href="#__codelineno-0-14"></a>    bdsk-url-2 = {https://doi.org/10.3390/s25061661}
<a id="__codelineno-0-15" name="__codelineno-0-15" href="#__codelineno-0-15"></a>}
</code></pre></div>
<h2 id="2024">2024</h2>
<h3 id="larger-encoders-smaller-regressors-exploring-label-dimensionality-reduction-and-multimodal-large-language-models-as-feature-extractors-for-predicting-social-perception">Larger Encoders, Smaller Regressors: Exploring Label Dimensionality Reduction and Multimodal Large Language Models as Feature Extractors for Predicting Social Perception</h3>
<p>Iván Martín-Fernández, Sergio Esteban-Romero, Jaime Bellver-Soler, Fernando Fernández-Martínez, and Manuel Gil-Martín. 2024. Larger Encoders, Smaller Regressors: Exploring Label Dimensionality Reduction and Multimodal Large Language Models as Feature Extractors for Predicting Social Perception. In Proceedings of the 5th on Multimodal Sentiment Analysis Challenge and Workshop: Social Perception and Humor (MuSe'24). Association for Computing Machinery, New York, NY, USA, 20–27. <a href="https://doi.org/10.1145/3689062.3689083">https://doi.org/10.1145/3689062.3689083</a></p>
<p><strong>Abstract</strong>:
Designing reliable automatic models for social perception can contribute to a better understanding of human behavior, enabling more trustworthy experiences in the multimedia on-line communication environment. However, predicting social attributes from video data remains challenging due to the complex interplay of visual, auditory, and linguistic cues. In this paper, we address this challenge by investigating the effectiveness of Multimodal Large Language Models (MM-LLMs) for feature extraction in the MuSe-Perception challenge. Firstly, our analysis of the novel LMU-ELP dataset has revealed high correlations between certain perceptual dimensions, motivating using a single regression model for all 16 social attributes to be predicted for a set of speakers appearing in recorded video clips. We demonstrate that dimensionality reduction through Principal Component Analysis (PCA) can be applied to the label space without a relevant performance loss. Secondly, by employing frozen MM-LLMs as feature extractors, we explore their ability to capture perception-related information. We extract sequence embeddings from the Qwen-VL and Qwen-Audio models and train a Multi-Layer Perceptron over the attention-pooled vectors for each one of the encoders, obtaining a mean Pearson correlation of 0.22 using the average predictions for both models. Our best result of 0.31 is achieved by training the same architecture over the baseline vit-ver and w2v-msp features, which motivates further exploration on how to effectively leverage advanced MM-LLMs as feature extractors. Lastly, a post hoc analysis of our results highlights the limitations of Pearson correlation for evaluating regression performance in this context. In particular, a similar Pearson coefficient can be obtained with two very different prediction sets displaying different levels of variability. We take this result as a call to action in exploring alternative metrics to assess the regression performance for the task.</p>
<div class="highlight"><span class="filename">Bibtex</span><pre><span></span><code><a id="__codelineno-1-1" name="__codelineno-1-1" href="#__codelineno-1-1"></a>@inproceedings{10.1145/3689062.3689083,
<a id="__codelineno-1-2" name="__codelineno-1-2" href="#__codelineno-1-2"></a>author = {Mart\&#39;{\i}n-Fern\&#39;{a}ndez, Iv\&#39;{a}n and Esteban-Romero, Sergio and Bellver-Soler, Jaime and Fern\&#39;{a}ndez-Mart\&#39;{\i}nez, Fernando and Gil-Mart\&#39;{\i}n, Manuel},
<a id="__codelineno-1-3" name="__codelineno-1-3" href="#__codelineno-1-3"></a>title = {Larger Encoders, Smaller Regressors: Exploring Label Dimensionality Reduction and Multimodal Large Language Models as Feature Extractors for Predicting Social Perception},
<a id="__codelineno-1-4" name="__codelineno-1-4" href="#__codelineno-1-4"></a>year = {2024},
<a id="__codelineno-1-5" name="__codelineno-1-5" href="#__codelineno-1-5"></a>isbn = {9798400711992},
<a id="__codelineno-1-6" name="__codelineno-1-6" href="#__codelineno-1-6"></a>publisher = {Association for Computing Machinery},
<a id="__codelineno-1-7" name="__codelineno-1-7" href="#__codelineno-1-7"></a>address = {New York, NY, USA},
<a id="__codelineno-1-8" name="__codelineno-1-8" href="#__codelineno-1-8"></a>url = {https://doi.org/10.1145/3689062.3689083},
<a id="__codelineno-1-9" name="__codelineno-1-9" href="#__codelineno-1-9"></a>doi = {10.1145/3689062.3689083},
<a id="__codelineno-1-10" name="__codelineno-1-10" href="#__codelineno-1-10"></a>abstract = {Designing reliable automatic models for social perception can contribute to a better understanding of human behavior, enabling more trustworthy experiences in the multimedia on-line communication environment. However, predicting social attributes from video data remains challenging due to the complex interplay of visual, auditory, and linguistic cues. In this paper, we address this challenge by investigating the effectiveness of Multimodal Large Language Models (MM-LLMs) for feature extraction in the MuSe-Perception challenge. Firstly, our analysis of the novel LMU-ELP dataset has revealed high correlations between certain perceptual dimensions, motivating using a single regression model for all 16 social attributes to be predicted for a set of speakers appearing in recorded video clips. We demonstrate that dimensionality reduction through Principal Component Analysis (PCA) can be applied to the label space without a relevant performance loss. Secondly, by employing frozen MM-LLMs as feature extractors, we explore their ability to capture perception-related information. We extract sequence embeddings from the Qwen-VL and Qwen-Audio models and train a Multi-Layer Perceptron over the attention-pooled vectors for each one of the encoders, obtaining a mean Pearson correlation of 0.22 using the average predictions for both models. Our best result of 0.31 is achieved by training the same architecture over the baseline vit-ver and w2v-msp features, which motivates further exploration on how to effectively leverage advanced MM-LLMs as feature extractors. Lastly, a post hoc analysis of our results highlights the limitations of Pearson correlation for evaluating regression performance in this context. In particular, a similar Pearson coefficient can be obtained with two very different prediction sets displaying different levels of variability. We take this result as a call to action in exploring alternative metrics to assess the regression performance for the task.},
<a id="__codelineno-1-11" name="__codelineno-1-11" href="#__codelineno-1-11"></a>booktitle = {Proceedings of the 5th on Multimodal Sentiment Analysis Challenge and Workshop: Social Perception and Humor},
<a id="__codelineno-1-12" name="__codelineno-1-12" href="#__codelineno-1-12"></a>pages = {20–27},
<a id="__codelineno-1-13" name="__codelineno-1-13" href="#__codelineno-1-13"></a>numpages = {8},
<a id="__codelineno-1-14" name="__codelineno-1-14" href="#__codelineno-1-14"></a>keywords = {affective computing, multimodal large language model., multimodal sentiment analysis, social perception},
<a id="__codelineno-1-15" name="__codelineno-1-15" href="#__codelineno-1-15"></a>location = {Melbourne VIC, Australia},
<a id="__codelineno-1-16" name="__codelineno-1-16" href="#__codelineno-1-16"></a>series = {MuSe&#39;24}
<a id="__codelineno-1-17" name="__codelineno-1-17" href="#__codelineno-1-17"></a>}
</code></pre></div>
<h2 id="2023">2023</h2>
<h3 id="exploring-video-transformers-and-automatic-segment-selection-for-memorability-prediciton">Exploring Video Transformers and Automatic Segment Selection for Memorability Prediciton</h3>
<p>Martín-Fernández, I., Esteban-Romero, S., Bellver-Soler, J., Gil-Martín, M., &amp; Fernández-Martínez, F. (2023). Exploring Video Transformers and Automatic Segment Selection for Memorability Prediction. Available at <a href="https://ceur-ws.org/Vol-3658/paper25.pdf">https://ceur-ws.org/Vol-3658/paper25.pdf</a></p>
<p><strong>Abstract</strong>:
This paper summarises THAU-UPM’s approach and results from the MediaEval 2023 Predicting Video Memorability task. Focused on the generalisation subtask, our work leverages a pre-trained Video Vision Transformer (ViViT), fine-tuned on memorability-related data, to model temporal and spatial relationships in videos. We propose novel, annotator-independent automatic segment selection methods grounded in visual saliency. These methods identify the most relevant video frames prior to conducting memorability score estimation. This selection process is implemented during both training and evaluation phases. Our study demonstrates the effectiveness of fine-tuning the ViViT model compared to a scratchtrained baseline, emphasising the importance of pre-training for predicting memorability. However, the model shows comparable sensitivity to both saliency-based and naive segment selection methods, suggesting that fine-tuning may harness similar benefits from various video segments. These results underscore the robustness of our approach but also signal the need for ongoing research.</p>
<div class="highlight"><span class="filename">Bibtex</span><pre><span></span><code><a id="__codelineno-2-1" name="__codelineno-2-1" href="#__codelineno-2-1"></a>@article{martin2023exploring,
<a id="__codelineno-2-2" name="__codelineno-2-2" href="#__codelineno-2-2"></a>  title={Exploring Video Transformers and Automatic Segment Selection for Memorability Prediction},
<a id="__codelineno-2-3" name="__codelineno-2-3" href="#__codelineno-2-3"></a>  author={Mart{\&#39;\i}n-Fern{\&#39;a}ndez, Iv{\&#39;a}n and Esteban-Romero, Sergio and Bellver-Soler, Jaime and Gil-Mart{\&#39;\i}n, Manuel and Fern{\&#39;a}ndez-Mart{\&#39;\i}nez, Fernando},
<a id="__codelineno-2-4" name="__codelineno-2-4" href="#__codelineno-2-4"></a>  year={2023}
<a id="__codelineno-2-5" name="__codelineno-2-5" href="#__codelineno-2-5"></a>}
</code></pre></div>
<h3 id="video-memorability-prediction-from-jointly-learnt-semantic-and-visual-features">Video Memorability Prediction From Jointly-learnt Semantic and Visual Features</h3>
<p>Iván Martín-Fernández, Ricardo Kleinlein, Cristina Luna-Jiménez, Manuel Gil-Martín, and Fernando Fernández-Martínez. 2023. Video Memorability Prediction From Jointly-learnt Semantic and Visual Features. In Proceedings of the 20th International Conference on Content-based Multimedia Indexing (CBMI '23). Association for Computing Machinery, New York, NY, USA, 178–182. <a href="https://doi.org/10.1145/3617233.3617260">https://doi.org/10.1145/3617233.3617260</a></p>
<p><strong>Abstract</strong>:
The memorability of a video is defined as an intrinsic property of its visual features that dictates the fraction of people who recall having watched it on a second viewing within a memory game. Still, unravelling what are the key features to predict memorability remains an obscure matter. This challenge is addressed here by fine-tuning text and image encoders using a cross-modal strategy known as Contrastive Language-Image Pre-training (CLIP). The resulting video-level data representations learned include semantics and topic-descriptive information as observed from both modalities, hence enhancing the predictive power of our algorithms. Our proposal achieves in the text domain a significantly greater Spearman Rank Correlation Coefficient (SRCC) than a default pre-trained text encoder (0.575 ± 0.007 and 0.538 ± 0.007, respectively) over the Memento10K dataset. A similar trend, although less pronounced, can be noticed in the visual domain. We believe these findings signal the potential benefits that cross-modal predictive systems can extract from being fine-tuned to the specific issue of media memorability.</p>
<div class="highlight"><span class="filename">Bibtex</span><pre><span></span><code><a id="__codelineno-3-1" name="__codelineno-3-1" href="#__codelineno-3-1"></a>@inproceedings{10.1145/3617233.3617260,
<a id="__codelineno-3-2" name="__codelineno-3-2" href="#__codelineno-3-2"></a>author = {Mart\&#39;{\i}n-Fern\&#39;{a}ndez, Iv\&#39;{a}n and Kleinlein, Ricardo and Luna-Jim\&#39;{e}nez, Cristina and Gil-Mart\&#39;{\i}n, Manuel and Fern\&#39;{a}ndez-Mart\&#39;{\i}nez, Fernando},
<a id="__codelineno-3-3" name="__codelineno-3-3" href="#__codelineno-3-3"></a>title = {Video Memorability Prediction From Jointly-learnt Semantic and Visual Features},
<a id="__codelineno-3-4" name="__codelineno-3-4" href="#__codelineno-3-4"></a>year = {2023},
<a id="__codelineno-3-5" name="__codelineno-3-5" href="#__codelineno-3-5"></a>isbn = {9798400709128},
<a id="__codelineno-3-6" name="__codelineno-3-6" href="#__codelineno-3-6"></a>publisher = {Association for Computing Machinery},
<a id="__codelineno-3-7" name="__codelineno-3-7" href="#__codelineno-3-7"></a>address = {New York, NY, USA},
<a id="__codelineno-3-8" name="__codelineno-3-8" href="#__codelineno-3-8"></a>url = {https://doi.org/10.1145/3617233.3617260},
<a id="__codelineno-3-9" name="__codelineno-3-9" href="#__codelineno-3-9"></a>doi = {10.1145/3617233.3617260},
<a id="__codelineno-3-10" name="__codelineno-3-10" href="#__codelineno-3-10"></a>booktitle = {Proceedings of the 20th International Conference on Content-Based Multimedia Indexing},
<a id="__codelineno-3-11" name="__codelineno-3-11" href="#__codelineno-3-11"></a>pages = {178–182},
<a id="__codelineno-3-12" name="__codelineno-3-12" href="#__codelineno-3-12"></a>numpages = {5},
<a id="__codelineno-3-13" name="__codelineno-3-13" href="#__codelineno-3-13"></a>keywords = {pre-training, media memorability, cross-modal, CLIP},
<a id="__codelineno-3-14" name="__codelineno-3-14" href="#__codelineno-3-14"></a>location = {&lt;conf-loc&gt;, &lt;city&gt;Orleans&lt;/city&gt;, &lt;country&gt;France&lt;/country&gt;, &lt;/conf-loc&gt;},
<a id="__codelineno-3-15" name="__codelineno-3-15" href="#__codelineno-3-15"></a>series = {CBMI &#39;23}
<a id="__codelineno-3-16" name="__codelineno-3-16" href="#__codelineno-3-16"></a>}
</code></pre></div>







  
  






                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
          <button type="button" class="md-top md-icon" data-md-component="top" hidden>
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"/></svg>
  Back to top
</button>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
        
<div class="md-social">
  
    
    
    
    
      
      
    
    <a href="https://scholar.google.com/citations?user=1eHvsbsAAAAJ&hl=es&oi=ao" target="_blank" rel="noopener" title="scholar.google.com" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M390.9 298.5s0 .1.1.1c9.2 19.4 14.4 41.1 14.4 64C405.3 445.1 338.5 512 256 512s-149.3-66.9-149.3-149.3c0-22.9 5.2-44.6 14.4-64 1.7-3.6 3.6-7.2 5.6-10.7q6.6-11.4 15-21.3c27.4-32.6 68.5-53.3 114.4-53.3 33.6 0 64.6 11.1 89.6 29.9 9.1 6.9 17.4 14.7 24.8 23.5 5.6 6.6 10.6 13.8 15 21.3 2 3.4 3.8 7 5.5 10.5zm26.4-18.8c-30.1-58.4-91-98.4-161.3-98.4s-131.2 40-161.3 98.4L0 202.7 256 0l256 202.7-94.7 77.1z"/></svg>
    </a>
  
    
    
    
    
    <a href="www.linkedin.com/in/ivan-martin-fernandez" target="_blank" rel="noopener" title="" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3M135.4 416H69V202.2h66.5V416zM102.2 96a38.5 38.5 0 1 1 0 77 38.5 38.5 0 1 1 0-77m282.1 320h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9z"/></svg>
    </a>
  
    
    
    
    
      
      
    
    <a href="https://github.com/imartinf" target="_blank" rel="noopener" title="github.com" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M173.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M252.8 8C114.1 8 8 113.3 8 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C436.2 457.8 504 362.9 504 252 504 113.3 391.5 8 252.8 8M105.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg>
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    
      
      <script id="__config" type="application/json">{"base": "..", "features": ["navigation.top"], "search": "../assets/javascripts/workers/search.973d3a69.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script>
    
    
      <script src="../assets/javascripts/bundle.f55a23d4.min.js"></script>
      
    
  </body>
</html>